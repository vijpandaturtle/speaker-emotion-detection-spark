{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBD Speech Emotion Recognition Part 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34_qCxrNBHq0",
        "outputId": "5e555cf2-7a0f-4ccb-ba9f-56dc5cf94ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech Emotion Recognition \n",
        "\n",
        "In this notebook we will load the spectrogram images and attempt to classify them with a pre-trained model."
      ],
      "metadata": {
        "id": "erI7MAw_BYIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_FOLDER = \"drive/MyDrive/UNI-AMRITA-SEM2/MLforBigData/MLBD_Project/MLBD_Dataset/Processed/\""
      ],
      "metadata": {
        "id": "4lmcb9oYBWyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "wLJCx8KQFux5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IndianAudioDataset(Dataset):\n",
        "  def __init__(self, base):\n",
        "    self.data = []\n",
        "    self.labels = []\n",
        "    self.categories = {\"calm\":0, \"anger\":1, \"fear\":2, \"sad\":3}\n",
        "    for emotion in os.listdir(base):\n",
        "      for clip in os.listdir(base+'/'+emotion):\n",
        "        self.labels.append(self.categories[emotion])\n",
        "    for emotion_dir in os.listdir(base):\n",
        "      for image in os.listdir(os.path.join(base,emotion_dir)):\n",
        "        self.data.append(read_image(os.path.join(base,emotion_dir,image)))\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "qVnBJK-jCnDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = IndianAudioDataset(IMAGES_FOLDER)\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "C_3cKK83Bm-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet34\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "resnet_model = resnet34(pretrained=True)\n",
        "resnet_model.fc = nn.Linear(512,50)\n",
        "resnet_model.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "resnet_model = resnet_model.to(device)"
      ],
      "metadata": {
        "id": "zOot01g4HCMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 2e-4\n",
        "optimizer = optim.Adam(resnet_model.parameters(), lr=learning_rate)\n",
        "epochs = 10\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "train_losses=[]\n",
        "train_acc = []\n",
        "def train(model, loss_fn, train_loader, epochs, optimizer, train_losses, train_acc, change_lr=None):\n",
        "  for epoch in range(1,epochs+1):\n",
        "    model.train()\n",
        "    batch_losses=[]\n",
        "    if change_lr:\n",
        "      optimizer = change_lr(optimizer, epoch)\n",
        "    running_loss=0\n",
        "    correct=0\n",
        "    total=0\n",
        "    for i, data in enumerate(train_loader):\n",
        "      x, y = data\n",
        "      optimizer.zero_grad()\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long)\n",
        "      y_hat = model(x)\n",
        "      loss = loss_fn(y_hat, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = y_hat.max(1)\n",
        "      total += y.size(0)\n",
        "      correct += predicted.eq(y).sum().item()\n",
        "    train_loss=running_loss/len(train_loader)\n",
        "    accu=100.*correct/total\n",
        "    train_acc.append(accu)\n",
        "    train_losses.append(train_loss)\n",
        "    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
        "train(resnet_model, loss_fn, train_loader, epochs, optimizer, train_losses, train_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8yQ1KLwHK4u",
        "outputId": "a2b7bfbf-987a-4234-9eba-f3267ec35284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.146 | Accuracy: 96.154\n",
            "Train Loss: 0.047 | Accuracy: 100.000\n",
            "Train Loss: 0.078 | Accuracy: 98.077\n",
            "Train Loss: 0.062 | Accuracy: 98.077\n",
            "Train Loss: 0.134 | Accuracy: 96.154\n",
            "Train Loss: 0.311 | Accuracy: 94.231\n",
            "Train Loss: 0.302 | Accuracy: 94.231\n",
            "Train Loss: 0.199 | Accuracy: 94.231\n",
            "Train Loss: 0.363 | Accuracy: 94.231\n",
            "Train Loss: 0.136 | Accuracy: 94.231\n"
          ]
        }
      ]
    }
  ]
}